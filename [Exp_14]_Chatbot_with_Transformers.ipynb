{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP+sBbGnWpvNNMAB4VpSDXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/EX/blob/main/%5BExp_14%5D_Chatbot_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14-14. 프로젝트: 한국어 데이터로 챗봇 만들기\n",
        "\n",
        "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다."
      ],
      "metadata": {
        "id": "e4YO1eZI_Kxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. 데이터 수집하기\n",
        "\n",
        "- 사용 데이터 셋 : 한국어 챗봇 데이터\n",
        "\n",
        "설명\n",
        "\n",
        "(1) 데이터 크기 : 11,876개\n",
        "\n",
        "(2) label : 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2"
      ],
      "metadata": {
        "id": "PAElP_S0_Wd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T65QMwWg5RyI",
        "outputId": "55b1d06d-440d-403e-f987-0eac0aaca3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 불러오기\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "9bHSoJTV4M07"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
        "train_data = pd.read_csv('ChatBotData.csv')\n",
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2RBEHUd85up9",
        "outputId": "c1e263b9-e087-4363-82de-865c20de9ded"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88b9ae0d-c8df-427f-9dde-26246f114b26\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88b9ae0d-c8df-427f-9dde-26246f114b26')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88b9ae0d-c8df-427f-9dde-26246f114b26 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88b9ae0d-c8df-427f-9dde-26246f114b26');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "\n",
        "# 챗봇 데이터 수 : 11,823개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkrnUNco53vT",
        "outputId": "9f973fb9-5f52-47e5-9e42-5614b2d4d7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 sample 확인\n",
        "\n",
        "train_data.sample(15)\n",
        "\n",
        "# sample을 사용하여 랜덤으로 text를 확인하고 전처리 방향을 생각해본다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "9OG_0FHq6CvJ",
        "outputId": "50c69d54-d263-4af2-a203-567a1fa0ae74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   Q                               A  label\n",
              "8400                     하늘이 준 이별 같다                          운명같네요.      1\n",
              "99                    거지 같이 일해 놓고 갔어  일 못하는 사람이 있으면 옆에 있는 사람이 더 힘들죠.      0\n",
              "2400                       생각없이 상처줬어                      생각하고 말하세요.      0\n",
              "9486            내가 더 좋아하는 거 같은 느낌이야.            그래봤자 상대방이 더 좋아할 거예요.      2\n",
              "3988                        잠을 잘못 잤나                         뻐근한가봐요.      0\n",
              "3404                        와인 한잔 짠~                          완전 좋죠.      0\n",
              "3644                       이 길이 맞을까?                    잘 가고 있을 거예요.      0\n",
              "10968                     잘 지냈으면 좋겠다                      따뜻한 마음이네요.      2\n",
              "2165                       분식 먹고 싶아.                 건강을 위해 조금씩 드세요.      0\n",
              "2886                      아이고 의미 없다.              인생은 욕망이지 의미가 아니에요.      0\n",
              "10038  상대가 먼저 만나자고 해서 만났는데 난 아닌 거 같대                   찔러보는 상대 별로네요.      2\n",
              "3971                              잘자                          내 꿈 꿔!      0\n",
              "766                 남편이 나보다 집안일 더 잘해                     이상적인 남편이네요.      0\n",
              "1075                          눈이 건조해                     눈을 깜빡거려보세요.      0\n",
              "7303                      원래 다 이런거겠죠                     누구나 그럴 거예요.      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b62978e-b5ec-45ee-8855-976b2b80ad6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8400</th>\n",
              "      <td>하늘이 준 이별 같다</td>\n",
              "      <td>운명같네요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>거지 같이 일해 놓고 갔어</td>\n",
              "      <td>일 못하는 사람이 있으면 옆에 있는 사람이 더 힘들죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2400</th>\n",
              "      <td>생각없이 상처줬어</td>\n",
              "      <td>생각하고 말하세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9486</th>\n",
              "      <td>내가 더 좋아하는 거 같은 느낌이야.</td>\n",
              "      <td>그래봤자 상대방이 더 좋아할 거예요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3988</th>\n",
              "      <td>잠을 잘못 잤나</td>\n",
              "      <td>뻐근한가봐요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404</th>\n",
              "      <td>와인 한잔 짠~</td>\n",
              "      <td>완전 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3644</th>\n",
              "      <td>이 길이 맞을까?</td>\n",
              "      <td>잘 가고 있을 거예요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10968</th>\n",
              "      <td>잘 지냈으면 좋겠다</td>\n",
              "      <td>따뜻한 마음이네요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2165</th>\n",
              "      <td>분식 먹고 싶아.</td>\n",
              "      <td>건강을 위해 조금씩 드세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2886</th>\n",
              "      <td>아이고 의미 없다.</td>\n",
              "      <td>인생은 욕망이지 의미가 아니에요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10038</th>\n",
              "      <td>상대가 먼저 만나자고 해서 만났는데 난 아닌 거 같대</td>\n",
              "      <td>찔러보는 상대 별로네요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3971</th>\n",
              "      <td>잘자</td>\n",
              "      <td>내 꿈 꿔!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>남편이 나보다 집안일 더 잘해</td>\n",
              "      <td>이상적인 남편이네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>눈이 건조해</td>\n",
              "      <td>눈을 깜빡거려보세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7303</th>\n",
              "      <td>원래 다 이런거겠죠</td>\n",
              "      <td>누구나 그럴 거예요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b62978e-b5ec-45ee-8855-976b2b80ad6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b62978e-b5ec-45ee-8855-976b2b80ad6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b62978e-b5ec-45ee-8855-976b2b80ad6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. 데이터 전처리하기\n",
        "\n",
        "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다.\n",
        "\n",
        "- step 1에서 여러번 샘플을 추출한 결과 Q, A가 자연스럽지 않거나, 문장끝에, ㅜ, ㅎ, .. 등 감정을 표현하는데 사용하는 초성 및 기호가 붙어있는 경우가있다.\n",
        "\n",
        "- 사례\n",
        "\n",
        "|idx|Q|A|label|\n",
        "|------|------|------|------|\n",
        "|5407|7년째|힘내세요.|1|\n",
        "|347|무거운 마음이네ㅎ|가벼워질 수만 있다면 좋을텐데요.|1|\n",
        "|2026|버려야 되는데..|잘 버리는 것도 중요해요.|0|\n",
        "|7321|으흠|에휴.|1|\n",
        "\n",
        "- 전체적으로 데이터의 특성을 파악 후 구체적인 전처리 함수를 구현해보도록한다."
      ],
      "metadata": {
        "id": "bAcT6M7n_Zxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측값 확인\n",
        "train_data.isnull().sum()\n",
        "\n",
        "# 결측치는 존재하지 않는다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSJDmXVT9zwd",
        "outputId": "9997a7f1-9202-4c9e-eb4e-a7e93e5f4505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Q        0\n",
              "A        0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장길이가 짧은 'Q' 데이터 출력\n",
        "train_data[train_data['Q'].str.len() == 1]\n",
        "\n",
        "# 'Q'의 길이가 짧지만 'A'가 어색하지는 않기 때문에 문장길이가 1이라 할지라도 삭제하지는 않기로 결정하였다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "RpGuewvdAA8C",
        "outputId": "f69eba7f-8633-46c5-cb95-d018d93b8d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Q           A  label\n",
              "2995  야           네      0\n",
              "3620  음           음      0\n",
              "3621  응           네      0\n",
              "5099  헉      놀랐나봐요.      0\n",
              "6909  야           네      1\n",
              "7228  왜      궁금하네요.      1\n",
              "8783  후     힘을 내세요.      1\n",
              "8806  휴         아이구      1\n",
              "8824  흠  마음이 복잡한가요.      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05dd8a28-cb6e-45cc-9da9-eb19a5c997bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>야</td>\n",
              "      <td>네</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3620</th>\n",
              "      <td>음</td>\n",
              "      <td>음</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3621</th>\n",
              "      <td>응</td>\n",
              "      <td>네</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5099</th>\n",
              "      <td>헉</td>\n",
              "      <td>놀랐나봐요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6909</th>\n",
              "      <td>야</td>\n",
              "      <td>네</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7228</th>\n",
              "      <td>왜</td>\n",
              "      <td>궁금하네요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8783</th>\n",
              "      <td>후</td>\n",
              "      <td>힘을 내세요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8806</th>\n",
              "      <td>휴</td>\n",
              "      <td>아이구</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8824</th>\n",
              "      <td>흠</td>\n",
              "      <td>마음이 복잡한가요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05dd8a28-cb6e-45cc-9da9-eb19a5c997bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05dd8a28-cb6e-45cc-9da9-eb19a5c997bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05dd8a28-cb6e-45cc-9da9-eb19a5c997bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장길이가 짧은 'A' 데이터 출력\n",
        "train_data[train_data['A'].str.len() == 1]\n",
        "\n",
        "# 위에 'Q'와 마찬가지로 'A'의 문장 길이가 1이라 할지라도 어색하지는 않기 때문에 삭제하지는 않기로 결정하였다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Pt3JhRhYAp_e",
        "outputId": "6e512852-f7b8-45b0-8408-89041e630a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Q  A  label\n",
              "2995     야  네      0\n",
              "3620     음  음      0\n",
              "3621     응  네      0\n",
              "3878   있잖아  음      0\n",
              "6909     야  네      1\n",
              "6910  야 너!  네      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8671111-c652-4962-89b5-b26efeffeb3e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>야</td>\n",
              "      <td>네</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3620</th>\n",
              "      <td>음</td>\n",
              "      <td>음</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3621</th>\n",
              "      <td>응</td>\n",
              "      <td>네</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3878</th>\n",
              "      <td>있잖아</td>\n",
              "      <td>음</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6909</th>\n",
              "      <td>야</td>\n",
              "      <td>네</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6910</th>\n",
              "      <td>야 너!</td>\n",
              "      <td>네</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8671111-c652-4962-89b5-b26efeffeb3e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8671111-c652-4962-89b5-b26efeffeb3e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8671111-c652-4962-89b5-b26efeffeb3e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 만들기\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "  # 예를 들어서 \"힘내세요.\" => \"힘내세요 .\"와 같이\n",
        "  # '힘내세요'와 온점 사이에 거리를 만듭니다.\n",
        "  sentence = re.sub(r'([?.!,])', r' \\1', sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "  # (0-9, ㄱ-ㅎ, ㅏ-ㅣ, 가-힣 , \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
        "  # 한글 데이터이기 모든 자음, 모음, 만들어지는 음절 조합을 포함시켜야한다.\n",
        "  sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣?.!,]+', ' ', sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "qLqm87bbBOog"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Q', 'A'의 쌍을 전처리 시행 후 \n",
        "Q = []\n",
        "for sentence_Q in train_data['Q']:\n",
        "  Q.append(preprocess_sentence(sentence_Q))\n",
        "\n",
        "A = []\n",
        "for sentence_A in train_data['A']:\n",
        "  A.append(preprocess_sentence(sentence_A))"
      ],
      "metadata": {
        "id": "-pd4wI_MEH__"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('전체 샘플 수 :', len(Q))\n",
        "print('전체 샘플 수 :', len(A))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJOX8aT8fDxH",
        "outputId": "5d6a13ff-b15d-4164-d9aa-2545608d194d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 11823\n",
            "전체 샘플 수 : 11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 'Q'와 'A'의 데이터 확인\n",
        "print(Q[:5])\n",
        "print(A[:5])\n",
        "\n",
        "# 의도한 전처리가 잘 진행되었다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydq8GVkLFJak",
        "outputId": "c2005e66-daad-4bd0-e0fe-0847bf866f59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', '심하네']\n",
            "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이 분포 출력\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Q_len = [len(s.split()) for s in Q]\n",
        "A_len = [len(s.split()) for s in A]\n",
        "\n",
        "print('Q의 최소 길이 : {}'.format(np.min(Q_len)))\n",
        "print('Q의 최대 길이 : {}'.format(np.max(Q_len)))\n",
        "print('Q의 평균 길이 : {}'.format(np.mean(Q_len)))\n",
        "print('A의 최소 길이 : {}'.format(np.min(A_len)))\n",
        "print('A의 최대 길이 : {}'.format(np.max(A_len)))\n",
        "print('A의 평균 길이 : {}'.format(np.mean(A_len)))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(Q_len)\n",
        "plt.title('Q')\n",
        "plt.subplot(1,2,2)\n",
        "plt.boxplot(A_len)\n",
        "plt.title('A')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.title('Q')\n",
        "plt.hist(Q_len, bins = 40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "plt.title('A')\n",
        "plt.hist(A_len, bins = 40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "# 기본적으로 'A'의 문자 최대 길이가 더 긴것으로 확인됐다.\n",
        "# 'Q'는 대부분 길이가 8 이하이고, 'A'도 대부분 길이가 8 이하이다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "nfXxbQYzOwwy",
        "outputId": "97bfaa77-c66d-42c8-b8e7-48fae7b57e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q의 최소 길이 : 1\n",
            "Q의 최대 길이 : 16\n",
            "Q의 평균 길이 : 3.9355493529561025\n",
            "A의 최소 길이 : 1\n",
            "A의 최대 길이 : 24\n",
            "A의 평균 길이 : 4.71521610420367\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. SubwordTextEncoder 사용하기\n",
        "\n",
        "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
      ],
      "metadata": {
        "id": "0PhyLGI0_dNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 단어장(Vocabulary) 만들기\n",
        "\n",
        "단어장을 만들 때는 질문과 답변 데이터셋을 모두 사용하여 만듭니다.\n",
        "\n",
        "[tensorflow tfds 설명](https://www.tensorflow.org/datasets/api_docs/python/tfds)"
      ],
      "metadata": {
        "id": "wtNh3ILFHlX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어장 만들기\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(Q + A, target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "BxAjQaR2HkSi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
      ],
      "metadata": {
        "id": "XjhjuAGmJAex"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작 토큰과 종료 토큰에 부여된 정수를 출력\n",
        "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etO3z0p2JFbz",
        "outputId": "53fc602b-15b8-472b-9530-842d71207537"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START_TOKEN의 번호 : [8164]\n",
            "END_TOKEN의 번호 : [8165]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각각 8,164와 8,165라는 점에서 현재 단어장의 크기가 8,164(0번부터 8,163번)이라는 의미입니다.\n",
        "# 두 개의 토큰을 추가해 주었기 때문에 단어장의 크기도 +2임을 명시해 주어야 합니다.\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rjPM7rWJNQU",
        "outputId": "e639bd50-d32a-41aa-91b6-3f796ad521a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작, 종료 토큰의 번호 및 단어 집합의 크기 출력\n",
        "print('시작 토큰 번호 :',START_TOKEN)\n",
        "print('종료 토큰 번호 :',END_TOKEN)\n",
        "print('단어 집합의 크기 :',VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LJgliAOJaZn",
        "outputId": "fe2d8b84-b617-47aa-eefc-184322e57b8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "시작 토큰 번호 : [8164]\n",
            "종료 토큰 번호 : [8165]\n",
            "단어 집합의 크기 : 8166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)\n",
        "\n",
        "tensorflow_datasets의 SubwordTextEncoder를 사용해서 tokenizer를 정의하고 Vocabulary를 만들었다면, tokenizer.encode()로 각 단어를 정수로 변환할 수 있고 또는 tokenizer.decode()를 통해 정수 시퀀스를 단어 시퀀스로 변환할 수 있습니다.\n",
        "\n",
        "예를 들어서 25번째 샘플을 tokenizer.encode()의 입력으로 사용해서 변환 결과를 봅시다"
      ],
      "metadata": {
        "id": "xgcs1Il-Jpck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의의 25번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
        "# 각 토큰을 고유한 정수로 변환\n",
        "print('정수 인코딩 후의 25번째 질문 샘플: {}'.format(tokenizer.encode(Q[25])))\n",
        "print('정수 인코딩 후의 25번째 답변 샘플: {}'.format(tokenizer.encode(A[25])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urpwADbBJrIe",
        "outputId": "a6d1cbc7-36c5-4da9-e190-780ebd21aae7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 25번째 질문 샘플: [5753, 429, 4132, 1055, 2]\n",
            "정수 인코딩 후의 25번째 답변 샘플: [1087, 3174, 591, 264, 878, 623, 287, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "임의의 질문 문장이 정수 시퀀스로 변환되었습니다. \n",
        "\n",
        " 반대로 정수 인코딩 된 결과는 다시 decode()를 사용하여 기존의 텍스트 시퀀스로 복원할 수 있습니다. \n",
        "\n",
        "샘플의 번호 20으로 설정한다."
      ],
      "metadata": {
        "id": "gpylKPf9J_U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\n",
        "# 임의의 입력 문장을 sample_string에 저장\n",
        "\n",
        "sample_string = Q[20]\n",
        "\n",
        "# encode() : 텍스트 시퀀스 > 정수 시퀀스\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# decode() : 정수 시퀀스 > 텍스트 시퀀스\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))\n",
        "\n",
        "# 인코딩 문장은 7개지만, 기존 문장은 4어절이다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHG5QTjIKHcc",
        "outputId": "7130af23-8ec3-412b-afe5-c090b816a7fc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 문장 [5761, 612, 3506, 141, 684, 3744, 847]\n",
            "기존 문장: 가스비 비싼데 감기 걸리겠어\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정수 인코딩 된 문장을 .decode()을 하면 자동으로 서브워드들까지 다시 붙여서 기존 단어로 복원해줍니다. \n",
        "\n",
        "아래 참고 문헌을 확인 후 분석시 주의하도록 한다.\n",
        "\n",
        "[NLP 챗봇 가이드](https://wikidocs.net/89786)"
      ],
      "metadata": {
        "id": "fvaVX5_wLN5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 정수는 각 단어와 어떻게 mapping되는지 병렬로 출력\n",
        "# 서브워드텍스트인코더는 의미있는 단위의 서브워드로 토크나이징한다. 띄어쓰기 단위 X 형태소 분석 단위 X\n",
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR9eo8cpLnj8",
        "outputId": "460e683e-cfcb-4157-d15e-21ed81c5167d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5761 ----> 가스\n",
            "612 ----> 비 \n",
            "3506 ----> 비싼\n",
            "141 ----> 데 \n",
            "684 ----> 감기 \n",
            "3744 ----> 걸리\n",
            "847 ----> 겠어\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 단어에 고유한 정수가 부여된 Vocabulary를 기준으로 단어 시퀀스가 정수 시퀀스로 인코딩된 결과를 확인할 수 있습니다. \n",
        "\n",
        "위의 결과와 마찬가지로 질문과 답변 셋에 대해서 전부 정수 인코딩을 수행합니다.\n",
        "\n",
        " 그 후 인코딩된 문장의 길이를 확인 후 적절한 최대 길이를 지정하고 해당 길이로 패딩을 합니다.\n"
      ],
      "metadata": {
        "id": "n6DPsjmHL8xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "    tokenized_inputs.append(sentence1)\n",
        "    tokenized_outputs.append(sentence2)\n",
        "    \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "  # 정수인코딩만 먼저 수행하고 인코딩된 문장의 길이를 먼저 확인해 볼것이다.\n",
        "  # 적절한 MAX_LENGTH 찾고 다시 인코딩을 진행하여도 학습에는 큰 문제가 없는 것으로 나타났다."
      ],
      "metadata": {
        "id": "scJmo15XPyCg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = tokenize_and_filter(Q, A)"
      ],
      "metadata": {
        "id": "8yeR3FaKQADu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서는 2가지 시도로 나뉜다.\n",
        "\n",
        "1. 모든 문장을 포함시키는 MAX_LENGTH 설정\n",
        "\n",
        "2. 많이 분포되어있는 LENGTH까지만 설정하기\n",
        "\n",
        "이 셀에서는 1번 방법을 사용하고 밑에서 추가적으로 2번 방법을 사용하여 학습을 시킬 것이다."
      ],
      "metadata": {
        "id": "sOECZWObIWbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAX_LENGTH를 정하기 위한 길이 분포 출력\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "questions_len = [len(s) for s in questions]\n",
        "answers_len = [len(s) for s in answers]\n",
        "\n",
        "print('questions_len의 최소 길이 : {}'.format(np.min(questions_len)))\n",
        "print('questions_len의 최대 길이 : {}'.format(np.max(questions_len)))\n",
        "print('questions_len의 평균 길이 : {}'.format(np.mean(questions_len)))\n",
        "print('answers_len의 최소 길이 : {}'.format(np.min(answers_len)))\n",
        "print('answers_len의 최대 길이 : {}'.format(np.max(answers_len)))\n",
        "print('answers_len의 평균 길이 : {}'.format(np.mean(answers_len)))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(questions_len)\n",
        "plt.title('questions_len')\n",
        "plt.subplot(1,2,2)\n",
        "plt.boxplot(answers_len)\n",
        "plt.title('answers_len')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.title('questions_len')\n",
        "plt.hist(questions_len, bins = 40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "plt.title('answers_len')\n",
        "plt.hist(answers_len, bins = 40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "# 'questions_len'와 'answers_len'의 길이 분포를 확인하고 적절한 MAX_LENGTH을 지정해야한다.\n",
        "# questions_len의 최대 길이가 23, answers_len의 최대 길이가 31이고 길이별 분포를 참고하여 MAX_LENGTH = 33로 설정한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "TbIfjbOoQWj2",
        "outputId": "940e93f8-c17c-42a7-ab29-c39a00b2d11b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "questions_len의 최소 길이 : 3\n",
            "questions_len의 최대 길이 : 23\n",
            "questions_len의 평균 길이 : 7.490822972172883\n",
            "answers_len의 최소 길이 : 3\n",
            "answers_len의 최대 길이 : 31\n",
            "answers_len의 평균 길이 : 7.817305252473991\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 길이를 33으로 정의\n",
        "MAX_LENGTH = 33\n",
        "\n",
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    # 최대 길이 33 이하인 경우에만 데이터셋으로 허용\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "\n",
        "  # 최대 길이 33으로 모든 데이터셋을 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen = MAX_LENGTH, padding = 'post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs"
      ],
      "metadata": {
        "id": "wR8Mozt7P1yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = tokenize_and_filter(Q, A)"
      ],
      "metadata": {
        "id": "XK_7sWhnSZSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 처리 결과 확인\n",
        "print('질문 데이터의 크기(shape) :', questions.shape)\n",
        "print('답변 데이터의 크기(shape) :', answers.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqlbHKsHSsH-",
        "outputId": "a7bcb950-d8fd-4e3e-969e-3ae9a1c760d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문 데이터의 크기(shape) : (11823, 33)\n",
            "답변 데이터의 크기(shape) : (11823, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 교사 강요(Teacher Forcing) 사용하기\n",
        "\n",
        "[위키독스 RNN 언어 모델](https://wikidocs.net/46496)\n",
        "\n",
        "Q.교사 강요(Teacher Forcing) 를 사용하지 않았을 경우, 훈련 과정에서 훈련 속도가 지나치게 느려지는 경우가 있다고 합니다. 그 이유는 무엇인가요?\n",
        "\n",
        "- 교사 강요를 하지 않은 경우, 잘못된 예측이 다음 시점(time step)의 입력으로 들어가면서 연쇄적으로 예측 정확도에 영향을 미친다.\n",
        "\n",
        "질문과 답변의 쌍을 tf.data.Dataset API의 입력으로 사용하여 파이프라인을 구성합니다. 이때, 교사 강요를 위해서 answers[:, :-1]를 디코더의 입력값, answers[:, 1:]를 디코더의 레이블로 사용합니다.\n"
      ],
      "metadata": {
        "id": "_WWVO5x1S9wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {'inputs': questions, 'dec_inputs': answers[:, :-1]},\n",
        "    {'outputs': answers[:, 1:]}))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "ZZ68K9gaTgbg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. 모델 구성하기\n",
        "\n",
        "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
      ],
      "metadata": {
        "id": "vU0w1b4l_hhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 포지셔널 인코딩 레이어\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # 각도 배열 생성\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "\n",
        "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # sin과 cosine이 교차되도록 재배열\n",
        "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
        "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
        "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "lwqr7HlLaJbC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # 가중치를 정규화\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # 패딩에 마스크 추가\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmax적용\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "  return output"
      ],
      "metadata": {
        "id": "Px9Cz1osaOtP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "Zz7ooKErZEDI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "YGICLOaoZEhV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "Gak0nfnoZcH0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # Q, K, V에 각각 Dense를 적용합니다\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # 스케일드 닷 프로덕트 어텐션 함수\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "OIfSM2joZfQl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # num_layers만큼 쌓아올린 인코더의 층.\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "0hE23lIMafZs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "\n",
        "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
        "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "XLZVHGhiai9z"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "\n",
        "  # 패딩 마스크\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "  \n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n"
      ],
      "metadata": {
        "id": "0g_X6rxVakl_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name = 'inputs')\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name = 'dec_inputs')\n",
        "\n",
        "  # 인코더에서 패딩을 위한 마스크\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "  # 디코더에서 패딩을 위한 마스크\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  # 인코더\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  # 디코더\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  # 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "6MUTklWDYTQR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
        "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.1 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh1MfzZaYUTw",
        "outputId": "82617865-3f1c-4989-e240-b5407e24aff0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, None, 256)    3144704     ['inputs[0][0]',                 \n",
            "                                                                  'enc_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
            "                                e)                                                                \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, None, 256)    3672064     ['dec_inputs[0][0]',             \n",
            "                                                                  'encoder[0][0]',                \n",
            "                                                                  'look_ahead_mask[0][0]',        \n",
            "                                                                  'dec_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, None, 8166)   2098662     ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,915,430\n",
            "Trainable params: 8,915,430\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 손실 함수(Loss function)\n",
        "\n",
        "레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 합니다."
      ],
      "metadata": {
        "id": "JuL3ZpgIjrE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "ybtDHiaTjtC9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 커스텀 된 학습률(Learning rate)\n",
        "\n",
        "딥러닝 모델학습 시 learning rate는 매우 중요한 하이퍼파라미터입니다. 최근에는 모델학습 초기에 learning rate를 급격히 높였다가, 이후 train step이 진행됨에 따라 서서히 낮추어 가면서 안정적으로 수렴하게 하는 고급 기법을 널리 사용하고 있습니다. 이런 방법을 커스텀 학습률 스케줄링(Custom Learning rate Scheduling)이라고 합니다.\n",
        "\n",
        "논문에 나온 공식을 참고하여 커스텀 학습률 스케줄러를 통한 아담 옵티마이저를 사용합니다. 논문에 나온 공식은 다음과 같습니다.\n",
        "\n",
        "![캡처.PNG](data:image/png;base64,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)\n",
        "\n"
      ],
      "metadata": {
        "id": "vVVHC7IMEItF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "WDQgfDNeEUV3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러면 방금 정의한 커스텀 학습률 스케줄링 계획을 시각화해 봅시다. 위에 언급한 수식은 step\\_num^{-0.5}step_num \n",
        "−0.5\n",
        " 에 비례하는 부분과 step\\_numstep_num에 비례하는 부분 중 작은 쪽을 택하도록 되어 있습니다. 그래서 학습 초기에는 learning_rate가 step\\_numstep_num에 비례해서 증가하다가 이후로는 감소하는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "D5n8MUFgGVoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "metadata": {
        "id": "omt5Vz7MGUzn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9d511dab-0b22-4a7d-a71b-3f9eb75f0f61"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 모델 컴파일\n",
        "\n",
        "손실 함수와 커스텀 된 학습률(learning rate)을 사용하여 모델을 컴파일합니다"
      ],
      "metadata": {
        "id": "urbo0EvzGYO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "srznFq5nGbkJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 훈련하기\n",
        "\n",
        "(1) 첫 번째 시도 : 에포크 20 적용\n",
        "\n",
        "(2) 두 번째 시도 : 에로크 100 적용"
      ],
      "metadata": {
        "id": "kRzICu0BGc70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 에포크 - 20\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "id": "YghvMTFwGfS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70b0c9e-7dbf-4244-b445-b8c19ba22fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "185/185 [==============================] - 18s 51ms/step - loss: 1.7760 - accuracy: 0.0260\n",
            "Epoch 2/20\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 1.4435 - accuracy: 0.0599\n",
            "Epoch 3/20\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 1.2249 - accuracy: 0.0618\n",
            "Epoch 4/20\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 1.1283 - accuracy: 0.0664\n",
            "Epoch 5/20\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 1.0572 - accuracy: 0.0705\n",
            "Epoch 6/20\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 0.9847 - accuracy: 0.0754\n",
            "Epoch 7/20\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.9052 - accuracy: 0.0827\n",
            "Epoch 8/20\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.8174 - accuracy: 0.0920\n",
            "Epoch 9/20\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.7210 - accuracy: 0.1025\n",
            "Epoch 10/20\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 0.6205 - accuracy: 0.1141\n",
            "Epoch 11/20\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.5185 - accuracy: 0.1267\n",
            "Epoch 12/20\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.4200 - accuracy: 0.1404\n",
            "Epoch 13/20\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 0.3290 - accuracy: 0.1535\n",
            "Epoch 14/20\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 0.2495 - accuracy: 0.1658\n",
            "Epoch 15/20\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.1838 - accuracy: 0.1776\n",
            "Epoch 16/20\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.1316 - accuracy: 0.1871\n",
            "Epoch 17/20\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.0958 - accuracy: 0.1937\n",
            "Epoch 18/20\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.0745 - accuracy: 0.1970\n",
            "Epoch 19/20\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0601 - accuracy: 0.1998\n",
            "Epoch 20/20\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0558 - accuracy: 0.2001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 에포크 - 100\n",
        "EPOCHS = 100\n",
        "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwkesFyoKZ6v",
        "outputId": "f684702d-4d68-4d73-e940-314ea82e6877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "185/185 [==============================] - 19s 50ms/step - loss: 1.7592 - accuracy: 0.0360\n",
            "Epoch 2/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 1.4352 - accuracy: 0.0602\n",
            "Epoch 3/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 1.2260 - accuracy: 0.0615\n",
            "Epoch 4/100\n",
            "185/185 [==============================] - 10s 54ms/step - loss: 1.1320 - accuracy: 0.0661\n",
            "Epoch 5/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 1.0612 - accuracy: 0.0699\n",
            "Epoch 6/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.9880 - accuracy: 0.0752\n",
            "Epoch 7/100\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.9067 - accuracy: 0.0829\n",
            "Epoch 8/100\n",
            "185/185 [==============================] - 10s 53ms/step - loss: 0.8177 - accuracy: 0.0924\n",
            "Epoch 9/100\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.7211 - accuracy: 0.1026\n",
            "Epoch 10/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.6197 - accuracy: 0.1143\n",
            "Epoch 11/100\n",
            "185/185 [==============================] - 10s 53ms/step - loss: 0.5186 - accuracy: 0.1267\n",
            "Epoch 12/100\n",
            "185/185 [==============================] - 10s 53ms/step - loss: 0.4200 - accuracy: 0.1402\n",
            "Epoch 13/100\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.3296 - accuracy: 0.1536\n",
            "Epoch 14/100\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 0.2492 - accuracy: 0.1659\n",
            "Epoch 15/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.1836 - accuracy: 0.1773\n",
            "Epoch 16/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.1338 - accuracy: 0.1862\n",
            "Epoch 17/100\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 0.0972 - accuracy: 0.1933\n",
            "Epoch 18/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0744 - accuracy: 0.1972\n",
            "Epoch 19/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0625 - accuracy: 0.1994\n",
            "Epoch 20/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0561 - accuracy: 0.2000\n",
            "Epoch 21/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0525 - accuracy: 0.2007\n",
            "Epoch 22/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0496 - accuracy: 0.2010\n",
            "Epoch 23/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0450 - accuracy: 0.2022\n",
            "Epoch 24/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0383 - accuracy: 0.2038\n",
            "Epoch 25/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0341 - accuracy: 0.2049\n",
            "Epoch 26/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0298 - accuracy: 0.2060\n",
            "Epoch 27/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0266 - accuracy: 0.2067\n",
            "Epoch 28/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0243 - accuracy: 0.2072\n",
            "Epoch 29/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0222 - accuracy: 0.2079\n",
            "Epoch 30/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0206 - accuracy: 0.2082\n",
            "Epoch 31/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0190 - accuracy: 0.2085\n",
            "Epoch 32/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0183 - accuracy: 0.2088\n",
            "Epoch 33/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0162 - accuracy: 0.2093\n",
            "Epoch 34/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0149 - accuracy: 0.2096\n",
            "Epoch 35/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0138 - accuracy: 0.2099\n",
            "Epoch 36/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0132 - accuracy: 0.2101\n",
            "Epoch 37/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0128 - accuracy: 0.2102\n",
            "Epoch 38/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0119 - accuracy: 0.2104\n",
            "Epoch 39/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0110 - accuracy: 0.2107\n",
            "Epoch 40/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0109 - accuracy: 0.2105\n",
            "Epoch 41/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0103 - accuracy: 0.2107\n",
            "Epoch 42/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0096 - accuracy: 0.2110\n",
            "Epoch 43/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0094 - accuracy: 0.2110\n",
            "Epoch 44/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0087 - accuracy: 0.2112\n",
            "Epoch 45/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0081 - accuracy: 0.2114\n",
            "Epoch 46/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0081 - accuracy: 0.2113\n",
            "Epoch 47/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0079 - accuracy: 0.2113\n",
            "Epoch 48/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0072 - accuracy: 0.2115\n",
            "Epoch 49/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0072 - accuracy: 0.2115\n",
            "Epoch 50/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0069 - accuracy: 0.2116\n",
            "Epoch 51/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0065 - accuracy: 0.2116\n",
            "Epoch 52/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0066 - accuracy: 0.2117\n",
            "Epoch 53/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0067 - accuracy: 0.2117\n",
            "Epoch 54/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0058 - accuracy: 0.2118\n",
            "Epoch 55/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0059 - accuracy: 0.2118\n",
            "Epoch 56/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0057 - accuracy: 0.2118\n",
            "Epoch 57/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0057 - accuracy: 0.2119\n",
            "Epoch 58/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0057 - accuracy: 0.2119\n",
            "Epoch 59/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0053 - accuracy: 0.2119\n",
            "Epoch 60/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0053 - accuracy: 0.2120\n",
            "Epoch 61/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0052 - accuracy: 0.2119\n",
            "Epoch 62/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0046 - accuracy: 0.2121\n",
            "Epoch 63/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0051 - accuracy: 0.2121\n",
            "Epoch 64/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0049 - accuracy: 0.2121\n",
            "Epoch 65/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0046 - accuracy: 0.2121\n",
            "Epoch 66/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0046 - accuracy: 0.2121\n",
            "Epoch 67/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0040 - accuracy: 0.2122\n",
            "Epoch 68/100\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0044 - accuracy: 0.2121\n",
            "Epoch 69/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0041 - accuracy: 0.2122\n",
            "Epoch 70/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0040 - accuracy: 0.2122\n",
            "Epoch 71/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0037 - accuracy: 0.2123\n",
            "Epoch 72/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0038 - accuracy: 0.2123\n",
            "Epoch 73/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0037 - accuracy: 0.2123\n",
            "Epoch 74/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0037 - accuracy: 0.2123\n",
            "Epoch 75/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0035 - accuracy: 0.2123\n",
            "Epoch 76/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0037 - accuracy: 0.2123\n",
            "Epoch 77/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0031 - accuracy: 0.2123\n",
            "Epoch 78/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0035 - accuracy: 0.2123\n",
            "Epoch 79/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0033 - accuracy: 0.2124\n",
            "Epoch 80/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0032 - accuracy: 0.2124\n",
            "Epoch 81/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0032 - accuracy: 0.2123\n",
            "Epoch 82/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0033 - accuracy: 0.2123\n",
            "Epoch 83/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0030 - accuracy: 0.2124\n",
            "Epoch 84/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0032 - accuracy: 0.2124\n",
            "Epoch 85/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0031 - accuracy: 0.2123\n",
            "Epoch 86/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0032 - accuracy: 0.2124\n",
            "Epoch 87/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0028 - accuracy: 0.2125\n",
            "Epoch 88/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0030 - accuracy: 0.2124\n",
            "Epoch 89/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0029 - accuracy: 0.2124\n",
            "Epoch 90/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0028 - accuracy: 0.2124\n",
            "Epoch 91/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0027 - accuracy: 0.2124\n",
            "Epoch 92/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0030 - accuracy: 0.2124\n",
            "Epoch 93/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0026 - accuracy: 0.2124\n",
            "Epoch 94/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0025 - accuracy: 0.2125\n",
            "Epoch 95/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0028 - accuracy: 0.2125\n",
            "Epoch 96/100\n",
            "185/185 [==============================] - 9s 49ms/step - loss: 0.0025 - accuracy: 0.2125\n",
            "Epoch 97/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0024 - accuracy: 0.2125\n",
            "Epoch 98/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0024 - accuracy: 0.2125\n",
            "Epoch 99/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0024 - accuracy: 0.2125\n",
            "Epoch 100/100\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0021 - accuracy: 0.2125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 오류 메시지\n",
        "\n",
        "- ValueError: Missing data for input \"inputs\". You passed a data dictionary with keys ['input', 'dec_inputs']. Expected the following keys: ['inputs', 'dec_inputs']\n",
        "\n",
        "해결법\n",
        "\n",
        "- keys에서 예상되어야 할 값이 아닌 다른 값이 들어있다는 것을 확인했고 단순 오타가 있는듯하여 전체 코드에서 input을 찾았디.\n",
        "\n",
        "- 확인해보니 교사 강요에서 inputs을 input으로 타이핑하여 생긴 오류였다.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "cNljqwvlm7aC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. 모델 평가하기\n",
        "\n",
        "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
      ],
      "metadata": {
        "id": "snzTMvil_kRw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HNFbI26q9zrb"
      },
      "outputs": [],
      "source": [
        "def decoder_inference(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 인퍼런스 단계\n",
        "  for i in range(MAX_LENGTH):\n",
        "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "    predictions = predictions[:, -1:, :]\n",
        "\n",
        "    # 현재 예측한 단어의 정수\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_sequence, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(sentence):\n",
        "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
        "  prediction = decoder_inference(sentence)\n",
        "\n",
        "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  print('입력 : {}'.format(sentence))\n",
        "  print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence"
      ],
      "metadata": {
        "id": "B0sSxXQioI7q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 삽입 후 출력 문장 확인해보기"
      ],
      "metadata": {
        "id": "6VhhC7oJoxI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input_sentence 만들기\n",
        "# random을 사용하여 무작위로 전처리 한 문장을 추출한다\n",
        "import random\n",
        "max = 11823\n",
        "input_sentence = []\n",
        "for i in range(1, 20):\n",
        "  number=random.randint(1,max)\n",
        "  input_sentence.append(Q[number])"
      ],
      "metadata": {
        "id": "wSslZWV-oM8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_sentence 결과 확인\n",
        "input_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIwrWmqqpgAe",
        "outputId": "5c0c8e2b-22e0-4fcb-fb42-3ba4ae722088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['고백',\n",
              " '나를 왜 좋아하는 걸까 ?',\n",
              " '사랑하는 사람은 딱 알아볼 수 있어 ?',\n",
              " '오늘은 파도가 잔잔하네',\n",
              " '남자친구가 변해가는거 같아 .',\n",
              " '여자친구 요구가 심해져',\n",
              " '나는 참 단순했다',\n",
              " '눈물만 .',\n",
              " '로또 번호 알려줘',\n",
              " '놀아 줄거지',\n",
              " '난 너한테 결코 네버 절대로 연락 안할거다 .',\n",
              " '배고파',\n",
              " '제2외국어 뭐 선택할까',\n",
              " '잠수이별이라니 !',\n",
              " '전학가도 잘 할 수 있겠지 ?',\n",
              " '이 무리에 낀게 잘못인가',\n",
              " '사랑해보고싶어',\n",
              " '이별할때 남자들의 거짓말',\n",
              " '심심해요']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q, A 출력 함수 구현\n",
        "\n",
        "def Q_A(input_sentence):\n",
        "  for sentence in input_sentence:\n",
        "    sentence_generation(sentence)\n",
        "    print()"
      ],
      "metadata": {
        "id": "eF5YjGcNqN8y"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q_A 출력 - 에포크 20\n",
        "Q_A(input_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhpxJ0PcqvGs",
        "outputId": "f8e504a2-d5a8-470f-d6dd-751867295810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 바쁘게 사는게 나쁘지는 않네\n",
            "출력 : 잡념을 없애는 데는 최고라고 생각해요 .\n",
            "\n",
            "입력 : 패턴 풀어볼까\n",
            "출력 : 할 수 있다면요 .\n",
            "\n",
            "입력 : 생각만 해도 마음이 무거워\n",
            "출력 : 긍정적인 생각을 해보세요 .\n",
            "\n",
            "입력 : 짜증만땅\n",
            "출력 : 짜증날 땐 짜장면\n",
            "\n",
            "입력 : 남자친구가 연락이 너무 늦어.\n",
            "출력 : 바쁜 걸 이해해보세요 .\n",
            "\n",
            "입력 : 오늘도잠을못이루네\n",
            "출력 : 잠은 정말 중요한데요 .\n",
            "\n",
            "입력 : 그런 친구 아니었는데 너무 귀찮게 하네\n",
            "출력 : 친구가 좋아하나봐요 .\n",
            "\n",
            "입력 : 분통 터지지만 포기안한다고 말했네요\n",
            "출력 : 마음이 힘들겠어요 .\n",
            "\n",
            "입력 : 안 좋아하는데 계속 만나는 거 가능?\n",
            "출력 : 사귀는 건 힘들어요 .\n",
            "\n",
            "입력 : 갑자기 차가워졌는데.\n",
            "출력 : 이유가 있을 거예요 .\n",
            "\n",
            "입력 : 섬유유연제 향 좋은거 사야겠지\n",
            "출력 : 향이 많은 걸 말해주죠 .\n",
            "\n",
            "입력 : 답답하네. 재회 하고 싶어\n",
            "출력 : 달라지지 않는다면 이대로가 나을지도 몰라요 .\n",
            "\n",
            "입력 : 여자친구랑 어디 놀러갈까?\n",
            "출력 : 연인과 함께면 어디든 좋아요 .\n",
            "\n",
            "입력 : 퇴근하고 뭐 하지\n",
            "출력 : 몸과 마음이 좀 쉬어야 할 것 같아요 .\n",
            "\n",
            "입력 : 2년 만났어\n",
            "출력 : 딱 좋을 때네요 .\n",
            "\n",
            "입력 : 여자친구가 연락이 안돼\n",
            "출력 : 별일 없을거예요 .\n",
            "\n",
            "입력 : 평일 저녁 데이트 장소 좀 추천해줘\n",
            "출력 : 야경이 예쁜 곳이라면요 .\n",
            "\n",
            "입력 : 엿같다.\n",
            "출력 : 벗어나는 게 좋겠네요 .\n",
            "\n",
            "입력 : 좋아하는 남자 표정이 너무 차가워서 다가가기 어려워.\n",
            "출력 : 표정만 그럴 수도 있어요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q_A 출력 - 에포크 100\n",
        "Q_A(input_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqiSeg49OoYk",
        "outputId": "95660366-9cd0-4fcd-bd2c-b568e79fdb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 고백\n",
            "출력 : 성공하길 바랄게요 .\n",
            "\n",
            "입력 : 나를 왜 좋아하는 걸까 ?\n",
            "출력 : 당신의 매력이 넘쳐날 거예요 .\n",
            "\n",
            "입력 : 사랑하는 사람은 딱 알아볼 수 있어 ?\n",
            "출력 : 감정은 감출수 있는게 아니에요 .\n",
            "\n",
            "입력 : 오늘은 파도가 잔잔하네\n",
            "출력 : 생각들을 정리하게 좋겠네요 .\n",
            "\n",
            "입력 : 남자친구가 변해가는거 같아 .\n",
            "출력 : 변하는 걸 보는 게 힘들겠어요 .\n",
            "\n",
            "입력 : 여자친구 요구가 심해져\n",
            "출력 : 모두 맞춰줄 필요는 없어요 .\n",
            "\n",
            "입력 : 나는 참 단순했다\n",
            "출력 : 단순한 게 가장 좋을 때가 있습니다 .\n",
            "\n",
            "입력 : 눈물만 .\n",
            "출력 : 지금은 우세요 .\n",
            "\n",
            "입력 : 로또 번호 알려줘\n",
            "출력 : 제가 알면 제가 먼저 했을 거예요 .\n",
            "\n",
            "입력 : 놀아 줄거지\n",
            "출력 : 지금 그러고 있어요 .\n",
            "\n",
            "입력 : 난 너한테 결코 네버 절대로 연락 안할거다 .\n",
            "출력 : 이 결심 변치마세요 .\n",
            "\n",
            "입력 : 배고파\n",
            "출력 : 얼른 맛난 음식 드세요 .\n",
            "\n",
            "입력 : 제2외국어 뭐 선택할까\n",
            "출력 : 요즘은 잘 안 배우는 언어도 좋은 거 같아요 .\n",
            "\n",
            "입력 : 잠수이별이라니 !\n",
            "출력 : 상대방에 대한 예의가 없네요 .\n",
            "\n",
            "입력 : 전학가도 잘 할 수 있겠지 ?\n",
            "출력 : 걱정하지 마세요 . 잘할 거예요 .\n",
            "\n",
            "입력 : 이 무리에 낀게 잘못인가\n",
            "출력 : 지금도 늦지 않았어요 .\n",
            "\n",
            "입력 : 사랑해보고싶어\n",
            "출력 : 할 수 있을 거예요 .\n",
            "\n",
            "입력 : 이별할때 남자들의 거짓말\n",
            "출력 : 비겁한 자들의 변명이에요 .\n",
            "\n",
            "입력 : 심심해요\n",
            "출력 : 저랑 놀아요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_sentence_v2 만들기\n",
        "# 기존 파일에서 순서대로 출력한다.\n",
        "input_sentence_v2= []\n",
        "for i in range(20):\n",
        "  input_sentence_v2.append(Q[i])"
      ],
      "metadata": {
        "id": "MYt90YGCRB8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_A(input_sentence_v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ApPOS5hRNQu",
        "outputId": "1409cf36-2133-47de-f432-f9d029fdb134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 12시 땡 !\n",
            "출력 : 하루가 또 가네요 .\n",
            "\n",
            "입력 : 1지망 학교 떨어졌어\n",
            "출력 : 위로해 드립니다 .\n",
            "\n",
            "입력 : 3박4일 놀러가고 싶다\n",
            "출력 : 여행은 언제나 좋죠 .\n",
            "\n",
            "입력 : 3박4일 정도 놀러가고 싶다\n",
            "출력 : 여행은 언제나 좋죠 .\n",
            "\n",
            "입력 : 심하네\n",
            "출력 : 눈살이 찌푸려지죠 .\n",
            "\n",
            "입력 : 카드 망가졌어\n",
            "출력 : 다시 새로 사는 게 마음 편해요 .\n",
            "\n",
            "입력 : 카드 안돼\n",
            "출력 : 다시 새로 사는 게 마음 편해요 .\n",
            "\n",
            "입력 : 맞팔 왜 안하지ㅠㅠ\n",
            "출력 : 잘 모르고 있을 수도 있어요 .\n",
            "\n",
            "입력 : 시간낭비인 거 아는데 매일 하는 중\n",
            "출력 : 시간을 정하고 해보세요 .\n",
            "\n",
            "입력 : 시간낭비인데 자꾸 보게됨\n",
            "출력 : 시간을 정하고 해보세요 .\n",
            "\n",
            "입력 : 보면 나만 빼고 다 행복해보여\n",
            "출력 : 자랑하는 자리니까요 .\n",
            "\n",
            "입력 : 가끔 궁금해\n",
            "출력 : 그 사람도 그럴 거예요 .\n",
            "\n",
            "입력 : 가끔 뭐하는지 궁금해\n",
            "출력 : 그 사람도 그럴 거예요 .\n",
            "\n",
            "입력 : 가끔은 혼자인게 좋다\n",
            "출력 : 혼자를 즐기세요 .\n",
            "\n",
            "입력 : 가난한 자의 설움\n",
            "출력 : 돈은 다시 들어올 거예요 .\n",
            "\n",
            "입력 : 가만 있어도 땀난다\n",
            "출력 : 땀을 식혀주세요 .\n",
            "\n",
            "입력 : 가상화폐 쫄딱 망함\n",
            "출력 : 어서 잊고 새출발 하세요 .\n",
            "\n",
            "입력 : 가스불 켜고 나갔어\n",
            "출력 : 빨리 집에 돌아가서 끄고 나오세요 .\n",
            "\n",
            "입력 : 가스불 켜놓고 나온거 같아\n",
            "출력 : 빨리 집에 돌아가서 끄고 나오세요 .\n",
            "\n",
            "입력 : 가스비 너무 많이 나왔다 .\n",
            "출력 : 다음 달에는 더 절약해봐요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "jfby038dRRqX",
        "outputId": "fdcd9f57-79ed-4826-e080-2f802ced5bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Q                    A  label\n",
              "0                    12시 땡!           하루가 또 가네요.      0\n",
              "1               1지망 학교 떨어졌어            위로해 드립니다.      0\n",
              "2              3박4일 놀러가고 싶다          여행은 언제나 좋죠.      0\n",
              "3           3박4일 정도 놀러가고 싶다          여행은 언제나 좋죠.      0\n",
              "4                   PPL 심하네           눈살이 찌푸려지죠.      0\n",
              "5                 SD카드 망가졌어   다시 새로 사는 게 마음 편해요.      0\n",
              "6                   SD카드 안돼   다시 새로 사는 게 마음 편해요.      0\n",
              "7            SNS 맞팔 왜 안하지ㅠㅠ     잘 모르고 있을 수도 있어요.      0\n",
              "8   SNS 시간낭비인 거 아는데 매일 하는 중        시간을 정하고 해보세요.      0\n",
              "9         SNS 시간낭비인데 자꾸 보게됨        시간을 정하고 해보세요.      0\n",
              "10      SNS보면 나만 빼고 다 행복해보여          자랑하는 자리니까요.      0\n",
              "11                   가끔 궁금해        그 사람도 그럴 거예요.      0\n",
              "12              가끔 뭐하는지 궁금해        그 사람도 그럴 거예요.      0\n",
              "13              가끔은 혼자인게 좋다            혼자를 즐기세요.      0\n",
              "14                가난한 자의 설움       돈은 다시 들어올 거예요.      0\n",
              "15               가만 있어도 땀난다            땀을 식혀주세요.      0\n",
              "16               가상화폐 쫄딱 망함       어서 잊고 새출발 하세요.      0\n",
              "17               가스불 켜고 나갔어  빨리 집에 돌아가서 끄고 나오세요.      0\n",
              "18           가스불 켜놓고 나온거 같아  빨리 집에 돌아가서 끄고 나오세요.      0\n",
              "19           가스비 너무 많이 나왔다.      다음 달에는 더 절약해봐요.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1655679-8c17-4da5-9484-164313a48da9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SD카드 망가졌어</td>\n",
              "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SD카드 안돼</td>\n",
              "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n",
              "      <td>잘 모르고 있을 수도 있어요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n",
              "      <td>시간을 정하고 해보세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
              "      <td>시간을 정하고 해보세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SNS보면 나만 빼고 다 행복해보여</td>\n",
              "      <td>자랑하는 자리니까요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>가끔 궁금해</td>\n",
              "      <td>그 사람도 그럴 거예요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>가끔 뭐하는지 궁금해</td>\n",
              "      <td>그 사람도 그럴 거예요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>가끔은 혼자인게 좋다</td>\n",
              "      <td>혼자를 즐기세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>가난한 자의 설움</td>\n",
              "      <td>돈은 다시 들어올 거예요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>가만 있어도 땀난다</td>\n",
              "      <td>땀을 식혀주세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>가상화폐 쫄딱 망함</td>\n",
              "      <td>어서 잊고 새출발 하세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>가스불 켜고 나갔어</td>\n",
              "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>가스불 켜놓고 나온거 같아</td>\n",
              "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>가스비 너무 많이 나왔다.</td>\n",
              "      <td>다음 달에는 더 절약해봐요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1655679-8c17-4da5-9484-164313a48da9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d1655679-8c17-4da5-9484-164313a48da9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d1655679-8c17-4da5-9484-164313a48da9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존에 사용한 챗봇 데이터에서 전처리한 문장을 넣어서 결과를 출력해보았다.\n",
        "\n",
        "입력과 출력의 결과가 자연스럽게 나타나 학습이 잘 된것처럼 보였으나 train 데이터를 넣은것과 큰 차이가 없다.\n",
        "\n",
        "새로운 문장 15개를 만들어 넣어볼 필요가있다.\n",
        "\n",
        "문장은 긍정문, 부정문으로 각각 1세트씩 15문장을 만들도록하겠다."
      ],
      "metadata": {
        "id": "2iOIDs_nOwvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15문장 임의로 만들어보기 - 긍정문 + 의문문 + 평서문\n",
        "# 전처리한 문장처럼 온점같은 것들은 전부 띄어쓰기를 넣었다.\n",
        "pos_sentence = ['나는 할 수 있다 .','우리는 해낼거에요 !','1등 상을 받았다 .','행복하다 .',\n",
        "                '내일이 기대된다 .','나는 당신을 좋아해요 .','함께 극복합시다 .','기뻐요 !',\n",
        "                '사랑해요 !!','즐거워요 ㅋㅋ','도와드릴까요 ?','영화 보실래요 ??',\n",
        "                '회의는 1시부터입니다 .','시험 준비 완료 !','NLP 어렵네요 .']"
      ],
      "metadata": {
        "id": "-bVt0EItRiw9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 긍정문 챗봇 결과 출력\n",
        "Q_A(pos_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5hRPBVpTUOK",
        "outputId": "3c5c3325-1f53-4c7d-c5d1-9903b5efb8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 나는 할 수 있다 .\n",
            "출력 : 친구가 들으면 서운해 할 수도 있겠어요 .\n",
            "\n",
            "입력 : 우리는 해낼거에요 !\n",
            "출력 : 사람은 모두 변덕스럽지요 .\n",
            "\n",
            "입력 : 1등 상을 받았다 .\n",
            "출력 : 제대로 헤어지지 못했군요 .\n",
            "\n",
            "입력 : 행복하다 .\n",
            "출력 : 당신도 행복하세요 .\n",
            "\n",
            "입력 : 내일이 기대된다 .\n",
            "출력 : 좋은 일이 생길 거예요 .\n",
            "\n",
            "입력 : 나는 당신을 좋아해요 .\n",
            "출력 : 좋은 생각 많이 하세요 .\n",
            "\n",
            "입력 : 함께 극복합시다 .\n",
            "출력 : 저도 잘 모르겠어요 .\n",
            "\n",
            "입력 : 기뻐요 !\n",
            "출력 : 지금보다 더 잘 살 거예요 .\n",
            "\n",
            "입력 : 사랑해요 !!\n",
            "출력 : 안녕 !\n",
            "\n",
            "입력 : 즐거워요 ㅋㅋ\n",
            "출력 : 할 일이 많은데 안하는 것이요 .\n",
            "\n",
            "입력 : 도와드릴까요 ?\n",
            "출력 : 그 사람이 좋아하는 것들을 알아보세요 .\n",
            "\n",
            "입력 : 영화 보실래요 ??\n",
            "출력 : 그게 좋죠 .\n",
            "\n",
            "입력 : 회의는 1시부터입니다 .\n",
            "출력 : 다음에 잘 하면 돼요 .\n",
            "\n",
            "입력 : 시험 준비 완료 !\n",
            "출력 : 흑역사는 흑역사일뿐이에요 .\n",
            "\n",
            "입력 : NLP 어렵네요 .\n",
            "출력 : 쉽지 않을 거예요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15문장 임의로 만들어보기 - 부정문 + 의문문 + 평서문\n",
        "# 전처리한 문장처럼 온점같은 것들은 전부 띄어쓰기를 넣었다.\n",
        "neg_sentence = ['불안해요 ㅠㅠ .','걱정이됩니다 .','내가 할 수 있을까 ?','고독합니다 .',\n",
        "                '헤어졌습니다 .','취업난이 심각해 !','도망가고 싶어','왜 나한테 그런말을 해 ?',\n",
        "                '나가 죽어라 !','넌 왜 그렇게 살아 ?','월요일이 좋아 ?','안녕하세요 ~',\n",
        "                '라면 먹고 갈래 ?','시험 끝 !','CV 어렵네요 .']"
      ],
      "metadata": {
        "id": "uXdgZTBUUEbk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 부정문 챗봇 결과 출력\n",
        "Q_A(neg_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyQ-TehSYurg",
        "outputId": "e9c5593e-a735-4366-a44f-9718b24a9ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 불안해요 ㅠㅠ .\n",
            "출력 : 두려워하지 않아도 돼요 .\n",
            "\n",
            "입력 : 걱정이됩니다 .\n",
            "출력 : 새로운 사랑을 찾아보세요 .\n",
            "\n",
            "입력 : 내가 할 수 있을까 ?\n",
            "출력 : 그럼요 . 걱정하지 마세요\n",
            "\n",
            "입력 : 고독합니다 .\n",
            "출력 : 혼자가 아니에요 .\n",
            "\n",
            "입력 : 헤어졌습니다 .\n",
            "출력 : 많이 힘들었겠어요 .\n",
            "\n",
            "입력 : 취업난이 심각해 !\n",
            "출력 : 자신의 능력을 믿어보세요 .\n",
            "\n",
            "입력 : 도망가고 싶어\n",
            "출력 : 저도 모르겠어요 .\n",
            "\n",
            "입력 : 왜 나한테 그런말을 해 ?\n",
            "출력 : 마음에 이끌리는대로 하세요 .\n",
            "\n",
            "입력 : 나가 죽어라 !\n",
            "출력 : 그 사람도 당신을 좋아하길 바랍니다 .\n",
            "\n",
            "입력 : 넌 왜 그렇게 살아 ?\n",
            "출력 : 소식에 귀를 기울이지 마세요 .\n",
            "\n",
            "입력 : 월요일이 좋아 ?\n",
            "출력 : 일주일이 즐겁겠어요 .\n",
            "\n",
            "입력 : 안녕하세요 ~\n",
            "출력 : 안녕하세요 .\n",
            "\n",
            "입력 : 라면 먹고 갈래 ?\n",
            "출력 : 저는 좋아요 .\n",
            "\n",
            "입력 : 시험 끝 !\n",
            "출력 : 어흥 ! ! 호랑이보다 무섭나요 ?\n",
            "\n",
            "입력 : CV 어렵네요 .\n",
            "출력 : 쉽지 않을 거예요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 추가적인 시도 \n",
        "\n",
        "(1) 에포크를 300까지 돌려본다.\n",
        "\n",
        "(2) 다시 위에서 만들어둔 pos, neg_sentence를 넣어 결과를 비교한다."
      ],
      "metadata": {
        "id": "BqBJEaaOaTur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 에포크 - 300\n",
        "EPOCHS = 300\n",
        "history_1 = model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "id": "EbH0fD4Rb3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 긍정문 + 의문문 + 평서문 챗봇 결과 출력\n",
        "Q_A(pos_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySiECgZ8hW3n",
        "outputId": "b6491626-a828-44d6-d343-f8b22ef5a295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 나는 할 수 있다 .\n",
            "출력 : 파이팅 !\n",
            "\n",
            "입력 : 우리는 해낼거에요 !\n",
            "출력 : 이제 기억을 놓아주세요 .\n",
            "\n",
            "입력 : 1등 상을 받았다 .\n",
            "출력 : 발등이 얼른 아물길 바랄게요 .\n",
            "\n",
            "입력 : 행복하다 .\n",
            "출력 : 당신도 행복하세요 .\n",
            "\n",
            "입력 : 내일이 기대된다 .\n",
            "출력 : 외로우니까 사람이다 .\n",
            "\n",
            "입력 : 나는 당신을 좋아해요 .\n",
            "출력 : 많이 벌수록 좋아요 .\n",
            "\n",
            "입력 : 함께 극복합시다 .\n",
            "출력 : 그렇게 생각하게된 계기가 있었나봐요 .\n",
            "\n",
            "입력 : 기뻐요 !\n",
            "출력 : 지금 옆에서 기뻐해주는 사람을 살펴보세요 .\n",
            "\n",
            "입력 : 사랑해요 !!\n",
            "출력 : 제가 곁에 있을게요 .\n",
            "\n",
            "입력 : 즐거워요 ㅋㅋ\n",
            "출력 : 할 수 있을 거예요 .\n",
            "\n",
            "입력 : 도와드릴까요 ?\n",
            "출력 : 꾸준히 치료하세요 .\n",
            "\n",
            "입력 : 영화 보실래요 ??\n",
            "출력 : 잘 하는 걸로 주세요 .\n",
            "\n",
            "입력 : 회의는 1시부터입니다 .\n",
            "출력 : 나중에 알아줄 거예요 .\n",
            "\n",
            "입력 : 시험 준비 완료 !\n",
            "출력 : 인연은 있어요 .\n",
            "\n",
            "입력 : NLP 어렵네요 .\n",
            "출력 : 쉽지 않을 거예요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 부정문 + 의문문 + 평서문 챗봇 결과 출력\n",
        "Q_A(neg_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcIpLTJWhWv_",
        "outputId": "b075aa47-43a8-437b-9ce8-3222d12aae3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 불안해요 ㅠㅠ .\n",
            "출력 : 두려워하지 않아도 돼요 .\n",
            "\n",
            "입력 : 걱정이됩니다 .\n",
            "출력 : 자신을 사랑해주세요 .\n",
            "\n",
            "입력 : 내가 할 수 있을까 ?\n",
            "출력 : 그럼요 . 걱정하지 마세요\n",
            "\n",
            "입력 : 고독합니다 .\n",
            "출력 : 혼자가 아니에요 .\n",
            "\n",
            "입력 : 헤어졌습니다 .\n",
            "출력 : 많이 힘들었겠어요 .\n",
            "\n",
            "입력 : 취업난이 심각해 !\n",
            "출력 : 자신의 능력을 믿어보세요 .\n",
            "\n",
            "입력 : 도망가고 싶어\n",
            "출력 : 응시는 해야죠 .\n",
            "\n",
            "입력 : 왜 나한테 그런말을 해 ?\n",
            "출력 : 잘 이겨내고 있네요 .\n",
            "\n",
            "입력 : 나가 죽어라 !\n",
            "출력 : 당신은 생각보다 많은 시간이 지났네요 .\n",
            "\n",
            "입력 : 넌 왜 그렇게 살아 ?\n",
            "출력 : 저는 위로봇입니다 .\n",
            "\n",
            "입력 : 월요일이 좋아 ?\n",
            "출력 : 일주일이 즐겁겠어요 .\n",
            "\n",
            "입력 : 안녕하세요 ~\n",
            "출력 : 안녕하세요 .\n",
            "\n",
            "입력 : 라면 먹고 갈래 ?\n",
            "출력 : 저는 좋아요 .\n",
            "\n",
            "입력 : 시험 끝 !\n",
            "출력 : 나중에 없애주세요 .\n",
            "\n",
            "입력 : CV 어렵네요 .\n",
            "출력 : 쉽지 않을 거예요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 추가적인 시도 2\n",
        "\n",
        "위에서 말했던 MAX_LENGTH 적용 기준은 2번으로 적용하여 분석을 실시하려한다.\n",
        "\n",
        "(1) MAX_LENGTH = 13으로 지정하고, epoch는 300으로 지정한다.\n",
        "\n",
        "(2) test에 사용하는 문장은 pos, neg_sentence를 그대로 사용한다.\n",
        "\n",
        "(3) transformer 코드는 위에서 실행하고 밑에서 학습을 진행하기로 결정, 중복된 코드가 너무 많아질 것을 우려하였기 때문이다."
      ],
      "metadata": {
        "id": "s4Uu62wvHrw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 길이를 15으로 정의\n",
        "# \n",
        "MAX_LENGTH = 13\n",
        "\n",
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    # 최대 길이 13 이하인 경우에만 데이터셋으로 허용\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "\n",
        "  # 최대 길이 13으로 모든 데이터셋을 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen = MAX_LENGTH, padding = 'post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs"
      ],
      "metadata": {
        "id": "G7wAPKfaIwCb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = tokenize_and_filter(Q, A)"
      ],
      "metadata": {
        "id": "5klsr4e-Jyr6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 처리 결과 확인\n",
        "print('질문 데이터의 크기(shape) :', questions.shape)\n",
        "print('답변 데이터의 크기(shape) :', answers.shape)\n",
        "\n",
        "# MAX_LENGTH = 33일때는 데이터가 11823개였다. 약 700개 가량이 줄어들었다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZg4smMJJznk",
        "outputId": "e3266e8c-98b1-4966-ba8a-d039010c60d3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문 데이터의 크기(shape) : (11144, 13)\n",
            "답변 데이터의 크기(shape) : (11144, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 에포크 - 300, MAX_LENGTH = 13\n",
        "EPOCHS = 300\n",
        "history_1 = model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDLrVrdmKy7h",
        "outputId": "ae38088a-f474-4aa3-a3f8-c7853263c853"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "175/175 [==============================] - 15s 34ms/step - loss: 4.5437 - accuracy: 0.0720\n",
            "Epoch 2/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 3.6838 - accuracy: 0.1603\n",
            "Epoch 3/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 3.1083 - accuracy: 0.1632\n",
            "Epoch 4/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 2.8498 - accuracy: 0.1734\n",
            "Epoch 5/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 2.6678 - accuracy: 0.1843\n",
            "Epoch 6/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 2.4882 - accuracy: 0.1972\n",
            "Epoch 7/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 2.2953 - accuracy: 0.2139\n",
            "Epoch 8/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 2.0840 - accuracy: 0.2362\n",
            "Epoch 9/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 1.8540 - accuracy: 0.2614\n",
            "Epoch 10/300\n",
            "175/175 [==============================] - 6s 37ms/step - loss: 1.6089 - accuracy: 0.2898\n",
            "Epoch 11/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 1.3605 - accuracy: 0.3203\n",
            "Epoch 12/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 1.1146 - accuracy: 0.3534\n",
            "Epoch 13/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.8862 - accuracy: 0.3854\n",
            "Epoch 14/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.6792 - accuracy: 0.4184\n",
            "Epoch 15/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.5053 - accuracy: 0.4471\n",
            "Epoch 16/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.3661 - accuracy: 0.4717\n",
            "Epoch 17/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.2649 - accuracy: 0.4901\n",
            "Epoch 18/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.1977 - accuracy: 0.5018\n",
            "Epoch 19/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.1583 - accuracy: 0.5086\n",
            "Epoch 20/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.1355 - accuracy: 0.5118\n",
            "Epoch 21/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.1230 - accuracy: 0.5146\n",
            "Epoch 22/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.1141 - accuracy: 0.5160\n",
            "Epoch 23/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.1151 - accuracy: 0.5151\n",
            "Epoch 24/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.1050 - accuracy: 0.5171\n",
            "Epoch 25/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0907 - accuracy: 0.5207\n",
            "Epoch 26/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0803 - accuracy: 0.5229\n",
            "Epoch 27/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0716 - accuracy: 0.5252\n",
            "Epoch 28/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0653 - accuracy: 0.5265\n",
            "Epoch 29/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0575 - accuracy: 0.5286\n",
            "Epoch 30/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0520 - accuracy: 0.5303\n",
            "Epoch 31/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0503 - accuracy: 0.5310\n",
            "Epoch 32/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0450 - accuracy: 0.5317\n",
            "Epoch 33/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0413 - accuracy: 0.5327\n",
            "Epoch 34/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0396 - accuracy: 0.5333\n",
            "Epoch 35/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0353 - accuracy: 0.5343\n",
            "Epoch 36/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0352 - accuracy: 0.5346\n",
            "Epoch 37/300\n",
            "175/175 [==============================] - 6s 37ms/step - loss: 0.0316 - accuracy: 0.5353\n",
            "Epoch 38/300\n",
            "175/175 [==============================] - 6s 37ms/step - loss: 0.0301 - accuracy: 0.5358\n",
            "Epoch 39/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0274 - accuracy: 0.5360\n",
            "Epoch 40/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0270 - accuracy: 0.5363\n",
            "Epoch 41/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0250 - accuracy: 0.5368\n",
            "Epoch 42/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0243 - accuracy: 0.5369\n",
            "Epoch 43/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0230 - accuracy: 0.5375\n",
            "Epoch 44/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0217 - accuracy: 0.5376\n",
            "Epoch 45/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0206 - accuracy: 0.5380\n",
            "Epoch 46/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0201 - accuracy: 0.5380\n",
            "Epoch 47/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0190 - accuracy: 0.5381\n",
            "Epoch 48/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0182 - accuracy: 0.5385\n",
            "Epoch 49/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0171 - accuracy: 0.5387\n",
            "Epoch 50/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0175 - accuracy: 0.5386\n",
            "Epoch 51/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0166 - accuracy: 0.5387\n",
            "Epoch 52/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0159 - accuracy: 0.5390\n",
            "Epoch 53/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0148 - accuracy: 0.5392\n",
            "Epoch 54/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0149 - accuracy: 0.5392\n",
            "Epoch 55/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0144 - accuracy: 0.5393\n",
            "Epoch 56/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0135 - accuracy: 0.5395\n",
            "Epoch 57/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0128 - accuracy: 0.5396\n",
            "Epoch 58/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0134 - accuracy: 0.5396\n",
            "Epoch 59/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0136 - accuracy: 0.5393\n",
            "Epoch 60/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0121 - accuracy: 0.5398\n",
            "Epoch 61/300\n",
            "175/175 [==============================] - 6s 37ms/step - loss: 0.0122 - accuracy: 0.5398\n",
            "Epoch 62/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0124 - accuracy: 0.5398\n",
            "Epoch 63/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0112 - accuracy: 0.5400\n",
            "Epoch 64/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0105 - accuracy: 0.5402\n",
            "Epoch 65/300\n",
            "175/175 [==============================] - 6s 37ms/step - loss: 0.0112 - accuracy: 0.5401\n",
            "Epoch 66/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0112 - accuracy: 0.5401\n",
            "Epoch 67/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0110 - accuracy: 0.5399\n",
            "Epoch 68/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0098 - accuracy: 0.5403\n",
            "Epoch 69/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0104 - accuracy: 0.5403\n",
            "Epoch 70/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0098 - accuracy: 0.5403\n",
            "Epoch 71/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0101 - accuracy: 0.5400\n",
            "Epoch 72/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0095 - accuracy: 0.5402\n",
            "Epoch 73/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0098 - accuracy: 0.5403\n",
            "Epoch 74/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0097 - accuracy: 0.5403\n",
            "Epoch 75/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0086 - accuracy: 0.5405\n",
            "Epoch 76/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0090 - accuracy: 0.5404\n",
            "Epoch 77/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0080 - accuracy: 0.5406\n",
            "Epoch 78/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0085 - accuracy: 0.5405\n",
            "Epoch 79/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0080 - accuracy: 0.5407\n",
            "Epoch 80/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0084 - accuracy: 0.5406\n",
            "Epoch 81/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0076 - accuracy: 0.5406\n",
            "Epoch 82/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0077 - accuracy: 0.5406\n",
            "Epoch 83/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0079 - accuracy: 0.5405\n",
            "Epoch 84/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0071 - accuracy: 0.5408\n",
            "Epoch 85/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0076 - accuracy: 0.5407\n",
            "Epoch 86/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0075 - accuracy: 0.5407\n",
            "Epoch 87/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0065 - accuracy: 0.5409\n",
            "Epoch 88/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0076 - accuracy: 0.5406\n",
            "Epoch 89/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0074 - accuracy: 0.5406\n",
            "Epoch 90/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0068 - accuracy: 0.5408\n",
            "Epoch 91/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0072 - accuracy: 0.5407\n",
            "Epoch 92/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0065 - accuracy: 0.5409\n",
            "Epoch 93/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0062 - accuracy: 0.5408\n",
            "Epoch 94/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0063 - accuracy: 0.5409\n",
            "Epoch 95/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0062 - accuracy: 0.5409\n",
            "Epoch 96/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0061 - accuracy: 0.5409\n",
            "Epoch 97/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0066 - accuracy: 0.5408\n",
            "Epoch 98/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0066 - accuracy: 0.5408\n",
            "Epoch 99/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0058 - accuracy: 0.5409\n",
            "Epoch 100/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0057 - accuracy: 0.5409\n",
            "Epoch 101/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0063 - accuracy: 0.5409\n",
            "Epoch 102/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0063 - accuracy: 0.5409\n",
            "Epoch 103/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0057 - accuracy: 0.5409\n",
            "Epoch 104/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0052 - accuracy: 0.5410\n",
            "Epoch 105/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0052 - accuracy: 0.5410\n",
            "Epoch 106/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0056 - accuracy: 0.5409\n",
            "Epoch 107/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0053 - accuracy: 0.5409\n",
            "Epoch 108/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0050 - accuracy: 0.5409\n",
            "Epoch 109/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0048 - accuracy: 0.5411\n",
            "Epoch 110/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0052 - accuracy: 0.5410\n",
            "Epoch 111/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0053 - accuracy: 0.5411\n",
            "Epoch 112/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0050 - accuracy: 0.5409\n",
            "Epoch 113/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0057 - accuracy: 0.5409\n",
            "Epoch 114/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0048 - accuracy: 0.5410\n",
            "Epoch 115/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0045 - accuracy: 0.5410\n",
            "Epoch 116/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0055 - accuracy: 0.5409\n",
            "Epoch 117/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0052 - accuracy: 0.5409\n",
            "Epoch 118/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0046 - accuracy: 0.5410\n",
            "Epoch 119/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0047 - accuracy: 0.5410\n",
            "Epoch 120/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0051 - accuracy: 0.5410\n",
            "Epoch 121/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0049 - accuracy: 0.5410\n",
            "Epoch 122/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0042 - accuracy: 0.5411\n",
            "Epoch 123/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0044 - accuracy: 0.5411\n",
            "Epoch 124/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0042 - accuracy: 0.5411\n",
            "Epoch 125/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0042 - accuracy: 0.5410\n",
            "Epoch 126/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0048 - accuracy: 0.5410\n",
            "Epoch 127/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0046 - accuracy: 0.5411\n",
            "Epoch 128/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0047 - accuracy: 0.5410\n",
            "Epoch 129/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0048 - accuracy: 0.5411\n",
            "Epoch 130/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0039 - accuracy: 0.5412\n",
            "Epoch 131/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0038 - accuracy: 0.5412\n",
            "Epoch 132/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0040 - accuracy: 0.5411\n",
            "Epoch 133/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0036 - accuracy: 0.5412\n",
            "Epoch 134/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0041 - accuracy: 0.5410\n",
            "Epoch 135/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0036 - accuracy: 0.5412\n",
            "Epoch 136/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0038 - accuracy: 0.5411\n",
            "Epoch 137/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0040 - accuracy: 0.5411\n",
            "Epoch 138/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0034 - accuracy: 0.5411\n",
            "Epoch 139/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0036 - accuracy: 0.5411\n",
            "Epoch 140/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0041 - accuracy: 0.5410\n",
            "Epoch 141/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0041 - accuracy: 0.5411\n",
            "Epoch 142/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0033 - accuracy: 0.5412\n",
            "Epoch 143/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0034 - accuracy: 0.5412\n",
            "Epoch 144/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0038 - accuracy: 0.5411\n",
            "Epoch 145/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0035 - accuracy: 0.5412\n",
            "Epoch 146/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0034 - accuracy: 0.5411\n",
            "Epoch 147/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0037 - accuracy: 0.5412\n",
            "Epoch 148/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0039 - accuracy: 0.5411\n",
            "Epoch 149/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0038 - accuracy: 0.5412\n",
            "Epoch 150/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0033 - accuracy: 0.5412\n",
            "Epoch 151/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0031 - accuracy: 0.5413\n",
            "Epoch 152/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0035 - accuracy: 0.5412\n",
            "Epoch 153/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0029 - accuracy: 0.5413\n",
            "Epoch 154/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0036 - accuracy: 0.5411\n",
            "Epoch 155/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0033 - accuracy: 0.5412\n",
            "Epoch 156/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0032 - accuracy: 0.5412\n",
            "Epoch 157/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0037 - accuracy: 0.5411\n",
            "Epoch 158/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0030 - accuracy: 0.5412\n",
            "Epoch 159/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0033 - accuracy: 0.5412\n",
            "Epoch 160/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0033 - accuracy: 0.5412\n",
            "Epoch 161/300\n",
            "175/175 [==============================] - 6s 37ms/step - loss: 0.0034 - accuracy: 0.5412\n",
            "Epoch 162/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0035 - accuracy: 0.5411\n",
            "Epoch 163/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0033 - accuracy: 0.5411\n",
            "Epoch 164/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0030 - accuracy: 0.5412\n",
            "Epoch 165/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0029 - accuracy: 0.5412\n",
            "Epoch 166/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0032 - accuracy: 0.5412\n",
            "Epoch 167/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0037 - accuracy: 0.5411\n",
            "Epoch 168/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0031 - accuracy: 0.5412\n",
            "Epoch 169/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0035 - accuracy: 0.5411\n",
            "Epoch 170/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0033 - accuracy: 0.5411\n",
            "Epoch 171/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0034 - accuracy: 0.5412\n",
            "Epoch 172/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0035 - accuracy: 0.5412\n",
            "Epoch 173/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0030 - accuracy: 0.5412\n",
            "Epoch 174/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0031 - accuracy: 0.5412\n",
            "Epoch 175/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 176/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0029 - accuracy: 0.5412\n",
            "Epoch 177/300\n",
            "175/175 [==============================] - 7s 37ms/step - loss: 0.0030 - accuracy: 0.5412\n",
            "Epoch 178/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0028 - accuracy: 0.5413\n",
            "Epoch 179/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0029 - accuracy: 0.5413\n",
            "Epoch 180/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0030 - accuracy: 0.5413\n",
            "Epoch 181/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0029 - accuracy: 0.5413\n",
            "Epoch 182/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0029 - accuracy: 0.5412\n",
            "Epoch 183/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0033 - accuracy: 0.5412\n",
            "Epoch 184/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0024 - accuracy: 0.5412\n",
            "Epoch 185/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0031 - accuracy: 0.5412\n",
            "Epoch 186/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0029 - accuracy: 0.5411\n",
            "Epoch 187/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0030 - accuracy: 0.5411\n",
            "Epoch 188/300\n",
            "175/175 [==============================] - 6s 37ms/step - loss: 0.0027 - accuracy: 0.5412\n",
            "Epoch 189/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 190/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0026 - accuracy: 0.5413\n",
            "Epoch 191/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0028 - accuracy: 0.5413\n",
            "Epoch 192/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0028 - accuracy: 0.5412\n",
            "Epoch 193/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0028 - accuracy: 0.5412\n",
            "Epoch 194/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 195/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0028 - accuracy: 0.5412\n",
            "Epoch 196/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0028 - accuracy: 0.5413\n",
            "Epoch 197/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0026 - accuracy: 0.5414\n",
            "Epoch 198/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0027 - accuracy: 0.5412\n",
            "Epoch 199/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0028 - accuracy: 0.5412\n",
            "Epoch 200/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0030 - accuracy: 0.5412\n",
            "Epoch 201/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0027 - accuracy: 0.5412\n",
            "Epoch 202/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0025 - accuracy: 0.5413\n",
            "Epoch 203/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 204/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0029 - accuracy: 0.5412\n",
            "Epoch 205/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0025 - accuracy: 0.5412\n",
            "Epoch 206/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0026 - accuracy: 0.5412\n",
            "Epoch 207/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 208/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 209/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0021 - accuracy: 0.5414\n",
            "Epoch 210/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0028 - accuracy: 0.5413\n",
            "Epoch 211/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0028 - accuracy: 0.5412\n",
            "Epoch 212/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0030 - accuracy: 0.5412\n",
            "Epoch 213/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0028 - accuracy: 0.5412\n",
            "Epoch 214/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 215/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0025 - accuracy: 0.5412\n",
            "Epoch 216/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0022 - accuracy: 0.5414\n",
            "Epoch 217/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 218/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0026 - accuracy: 0.5413\n",
            "Epoch 219/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0025 - accuracy: 0.5412\n",
            "Epoch 220/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0025 - accuracy: 0.5412\n",
            "Epoch 221/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 222/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 223/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 224/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 225/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0026 - accuracy: 0.5413\n",
            "Epoch 226/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 227/300\n",
            "175/175 [==============================] - 6s 37ms/step - loss: 0.0025 - accuracy: 0.5413\n",
            "Epoch 228/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0030 - accuracy: 0.5411\n",
            "Epoch 229/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 230/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0026 - accuracy: 0.5413\n",
            "Epoch 231/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0025 - accuracy: 0.5412\n",
            "Epoch 232/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0025 - accuracy: 0.5412\n",
            "Epoch 233/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0026 - accuracy: 0.5413\n",
            "Epoch 234/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 235/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5412\n",
            "Epoch 236/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0025 - accuracy: 0.5412\n",
            "Epoch 237/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 238/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 239/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5412\n",
            "Epoch 240/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 241/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 242/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 243/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 244/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5414\n",
            "Epoch 245/300\n",
            "175/175 [==============================] - 6s 35ms/step - loss: 0.0026 - accuracy: 0.5413\n",
            "Epoch 246/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0022 - accuracy: 0.5413\n",
            "Epoch 247/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0024 - accuracy: 0.5412\n",
            "Epoch 248/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0021 - accuracy: 0.5414\n",
            "Epoch 249/300\n",
            "175/175 [==============================] - 6s 34ms/step - loss: 0.0020 - accuracy: 0.5413\n",
            "Epoch 250/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0025 - accuracy: 0.5413\n",
            "Epoch 251/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0022 - accuracy: 0.5414\n",
            "Epoch 252/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 253/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5414\n",
            "Epoch 254/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 255/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0027 - accuracy: 0.5413\n",
            "Epoch 256/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 257/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 258/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5413\n",
            "Epoch 259/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0020 - accuracy: 0.5413\n",
            "Epoch 260/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 261/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0022 - accuracy: 0.5413\n",
            "Epoch 262/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 263/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 264/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0026 - accuracy: 0.5413\n",
            "Epoch 265/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 266/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5414\n",
            "Epoch 267/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 268/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 269/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0020 - accuracy: 0.5414\n",
            "Epoch 270/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0020 - accuracy: 0.5413\n",
            "Epoch 271/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 272/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0022 - accuracy: 0.5413\n",
            "Epoch 273/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 274/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5414\n",
            "Epoch 275/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 276/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0022 - accuracy: 0.5413\n",
            "Epoch 277/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0025 - accuracy: 0.5412\n",
            "Epoch 278/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0028 - accuracy: 0.5412\n",
            "Epoch 279/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0019 - accuracy: 0.5414\n",
            "Epoch 280/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 281/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 282/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0022 - accuracy: 0.5413\n",
            "Epoch 283/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5414\n",
            "Epoch 284/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0020 - accuracy: 0.5413\n",
            "Epoch 285/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0024 - accuracy: 0.5412\n",
            "Epoch 286/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 287/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0020 - accuracy: 0.5413\n",
            "Epoch 288/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0023 - accuracy: 0.5413\n",
            "Epoch 289/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 290/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 291/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0020 - accuracy: 0.5414\n",
            "Epoch 292/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0022 - accuracy: 0.5413\n",
            "Epoch 293/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0022 - accuracy: 0.5413\n",
            "Epoch 294/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0022 - accuracy: 0.5413\n",
            "Epoch 295/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0019 - accuracy: 0.5413\n",
            "Epoch 296/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 297/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0020 - accuracy: 0.5414\n",
            "Epoch 298/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n",
            "Epoch 299/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0019 - accuracy: 0.5413\n",
            "Epoch 300/300\n",
            "175/175 [==============================] - 6s 36ms/step - loss: 0.0021 - accuracy: 0.5413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 긍정문 + 의문문 + 평서문 챗봇 결과 출력\n",
        "Q_A(pos_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAnktLhsMjra",
        "outputId": "1ee9badc-874a-4e80-8fc5-7785a86fdb67"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 나는 할 수 있다 .\n",
            "출력 : 파이팅 !\n",
            "\n",
            "입력 : 우리는 해낼거에요 !\n",
            "출력 : 이제 기억을 놓아주세요 .\n",
            "\n",
            "입력 : 1등 상을 받았다 .\n",
            "출력 : 그런 때라고 생각해요 .\n",
            "\n",
            "입력 : 행복하다 .\n",
            "출력 : 당신도 예뻐요 .\n",
            "\n",
            "입력 : 내일이 기대된다 .\n",
            "출력 : 좋은 일이 생길 거예요 .\n",
            "\n",
            "입력 : 나는 당신을 좋아해요 .\n",
            "출력 : 멍 때리기\n",
            "\n",
            "입력 : 함께 극복합시다 .\n",
            "출력 : 사랑을 재다보면 스스로가 더 힘들거예요 .\n",
            "\n",
            "입력 : 기뻐요 !\n",
            "출력 : 가끔씩 스트레칭을 해주세요 .\n",
            "\n",
            "입력 : 사랑해요 !!\n",
            "출력 : 많이 벌수록 좋아요 .\n",
            "\n",
            "입력 : 즐거워요 ㅋㅋ\n",
            "출력 : 할 수 있을 거예요 .\n",
            "\n",
            "입력 : 도와드릴까요 ?\n",
            "출력 : 뭐 더 잘 챙겨드세요 .\n",
            "\n",
            "입력 : 영화 보실래요 ??\n",
            "출력 : 들어드릴게요 .\n",
            "\n",
            "입력 : 회의는 1시부터입니다 .\n",
            "출력 : 다음에 잘 하면 돼요 .\n",
            "\n",
            "입력 : 시험 준비 완료 !\n",
            "출력 : 솔직한 마음으로 다가가보세요 .\n",
            "\n",
            "입력 : NLP 어렵네요 .\n",
            "출력 : 쉽지 않을 거예요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 부정문 + 의문문 + 평서문 챗봇 결과 출력\n",
        "Q_A(neg_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxeXSMGhMoSp",
        "outputId": "bb08622a-d7cc-4522-90f3-62785cc2b085"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 불안해요 ㅠㅠ .\n",
            "출력 : 두려워하지 않아도 돼요 .\n",
            "\n",
            "입력 : 걱정이됩니다 .\n",
            "출력 : 더 그렇게 만들면 돼요 .\n",
            "\n",
            "입력 : 내가 할 수 있을까 ?\n",
            "출력 : 그럼요 . 걱정하지 마세요\n",
            "\n",
            "입력 : 고독합니다 .\n",
            "출력 : 혼자가 아니에요 .\n",
            "\n",
            "입력 : 헤어졌습니다 .\n",
            "출력 : 많이 힘들었겠어요 .\n",
            "\n",
            "입력 : 취업난이 심각해 !\n",
            "출력 : 간절한 만큼 할 수 있을 거예요 .\n",
            "\n",
            "입력 : 도망가고 싶어\n",
            "출력 : 응시는 해야죠 .\n",
            "\n",
            "입력 : 왜 나한테 그런말을 해 ?\n",
            "출력 : 후유증이 클 수도 있겠어요 .\n",
            "\n",
            "입력 : 나가 죽어라 !\n",
            "출력 : 나쁜 생각은 버리세요 .\n",
            "\n",
            "입력 : 넌 왜 그렇게 살아 ?\n",
            "출력 : 소식에 귀를 기울이지 마세요 .\n",
            "\n",
            "입력 : 월요일이 좋아 ?\n",
            "출력 : 길은 항상 막히는 것 같아요 .\n",
            "\n",
            "입력 : 안녕하세요 ~\n",
            "출력 : 안녕하세요 .\n",
            "\n",
            "입력 : 라면 먹고 갈래 ?\n",
            "출력 : 저는 좋아요 .\n",
            "\n",
            "입력 : 시험 끝 !\n",
            "출력 : 이제 아프게 하면 그렇죠 .\n",
            "\n",
            "입력 : CV 어렵네요 .\n",
            "출력 : 쉽지 않을 거예요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 질의 응답 우수 및 미흡 결과 정리\n",
        "\n",
        "1. MAX_LENGTH = 33, epoch = 300\n",
        "\n",
        "(1) 긍정문 + 의문문 + 평서문 우수 및 미흡 결과\n",
        "\n",
        "|번호|입력(우수)|출력|입력(미흡)|출력|\n",
        "|------|------|------|------|------|\n",
        "|1|나는 할 수 있다 .|파이팅 !|우리는 해낼거에요 !|이제 기억을 놓아주세요 .|\n",
        "|2|사랑해요 !!|제가 곁에 있을게요 .|내일이 기대된다 .|외로우니까 사람이다 .|\n",
        "|3|행복하다 .|제가 곁에 있을게요 .|나는 당신을 좋아해요 .|많이 벌수록 좋아요 .|\n",
        "|4|행복하다 .|당신도 행복하세요|회의는 1시부터입니다 .|나중에 알아줄 거예요 .|\n",
        "\n",
        "(2) 부정문 + 의문문 + 평서문 우수 및 미흡 결과\n",
        "\n",
        "|번호|입력(우수)|출력|입력(미흡)|출력|\n",
        "|------|------|------|------|------|\n",
        "|1|불안해요 ㅠㅠ .|두려워하지 않아도 돼요 .|나가 죽어라 !|당신은 생각보다 많은 시간이 지났네요 .|\n",
        "|2|내가 할 수 있을까 ?|그럼요 . 걱정하지 마세요|넌 왜 그렇게 살아 ?|저는 위로봇입니다 .|\n",
        "|3|고독합니다 . |혼자가 아니에요 .|왜 나한테 그런말을 해 ?|잘 이겨내고 있네요 .|\n",
        "|4|라면 먹고 갈래 ?|저는 좋아요 .|월요일이 좋아 ?|일주일이 즐겁겠어요 .|\n",
        "|5|CV 어렵네요 . |쉽지 않을 거예요 .|시험 끝 !|나중에 없애주세요 .|\n",
        "\n",
        "2.  MAX_LENGTH = 13, epoch = 300\n",
        "\n",
        "(1) 긍정문 + 의문문 + 평서문 우수 및 미흡 결과\n",
        "\n",
        "|번호|입력(우수)|출력|입력(미흡)|출력|\n",
        "|------|------|------|------|------|\n",
        "|1|내일이 기대된다 .|좋은 일이 생길 거예요 .|우리는 해낼거에요 !|이제 기억을 놓아주세요|\n",
        "|2|NLP 어렵네요 .|쉽지 않을 거예요 .|나는 당신을 좋아해요 .|멍 때리기|\n",
        "|3|나는 할 수 있다 .파이팅 !|시험 준비 완료 !|솔직한 마음으로 다가가보세요 .|\n",
        "|4|-|-|함께 극복합시다 .|사랑을 재다보면 스스로가 더 힘들거예요 .|\n",
        "\n",
        "(2) 부정문 + 의문문 + 평서문 우수 및 미흡 결과\n",
        "\n",
        "|번호|입력(우수)|출력|입력(미흡)|출력|\n",
        "|------|------|------|------|------|\n",
        "|1|불안해요 ㅠㅠ .|두려워하지 않아도 돼요 .|걱정이됩니다 .|더 그렇게 만들면 돼요 .|\n",
        "|2|내가 할 수 있을까 ?|그럼요 . 걱정하지 마세요|월요일이 좋아 ?|길은 항상 막히는 것 같아요 .|\n",
        "|3|취업난이 심각해 !|간절한 만큼 할 수 있을 거예요 .|시험 끝 !|이제 아프게 하면 그렇죠 .|\n",
        "\n",
        "3. Loss, Acuraccy 비교\n",
        "\n",
        "||MAX_LENGTH : 33, EPOCH : 300|MAX_LENGTH : 33, EPOCH : 300|\n",
        "|------|------|------|\n",
        "|Loss|8.5860e-04|0.2127|\n",
        "|Acuraccy|0.0022|0.5413|\n",
        "\n",
        "4. 결론\n",
        "\n",
        "(1) 챗봇 성능의 만족도는 MAX_LENGTH가 33일 때 더 만족도가 높았다.\n",
        "\n",
        "(2) 최대 문자 길이가 줄었을 때 정확도는 올라갔지만 챗봇의 결과가 더 개선되지는 않았다.\n",
        "\n",
        "(3) 결국 모든 문장을 전부 넣을 수 있는 최대 문자 길이를 지정해야 더 좋은 결과를 가져온다고 추측할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "S0lzdtbEEwy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고"
      ],
      "metadata": {
        "id": "lX7qGW4XKysW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 이번 프로젝트에서 어려웠던 점\n",
        " \n",
        " (1) 트랜스포머의 구조를 이해하는게 쉽지 않았습니다. 모델을 바꾸기에는 힘이들어 LMS에 있는 코드를 그대로 사용하였습니다."
      ],
      "metadata": {
        "id": "ojmxzLRyK1cS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 프로젝트를 진행하면서 알게된 부분 또는 아직 이해하지 못한 부분\n",
        "\n",
        " (1) 챗봇의 성능 평가를 해보고 싶었지만 문장 요약처럼 사용할 수 있는 평가지표가 보이지 않았습니다. 다만 향후 챗봇을 만들고 어떻게 모니터링 해야하는지 찾아보았다. [Chatbot Analytics: 14 Chatbot Metrics To Track in 2022](https://botscrew.com/blog/chatbot-metrics/)\n",
        "\n",
        " (2) [Exp-14 LMS 내용 정리](https://github.com/sgr1118/FD/blob/main/Chatbot_with_Transformers_theroy.ipynb)"
      ],
      "metadata": {
        "id": "cMgNvnfQLhyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참조"
      ],
      "metadata": {
        "id": "xWtviva-f2gG"
      }
    }
  ]
}